{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5cd5061f-ae30-4837-a53b-690ffd5c5830",
    "_uuid": "9d82bf13584b8e682962fbb96131f2447d741679"
   },
   "source": [
    "# Mettre en place notre environnement\n",
    "________\n",
    "\n",
    "La première chose que nous devons faire est de charger les bibliothèques et les ensembles de données que nous allons utiliser. Pour aujourd'hui, nous allons travailler avec deux ensembles de données : un contenant des chroniques de température dans le lit de la rivière (zone hyporhéique), et un autre contenant des les chroniques de différence de charge enregistré en **Volt**.\n",
    "\n",
    "**Important!** Assurez-vous d'exécuter cette cellule vous-même ou le reste de votre code ne fonctionnera pas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the first line of the file \n",
    "T_measures = \"../sampling_points/Point034/point034_T_measures.csv\"\n",
    "with open(T_measures, 'r') as file:\n",
    "    all_lines = file.readlines()\n",
    "    file.close()\n",
    "    all_lines[0] = '#,\"Date Heure, GMT+01:00\",\"Temp., °C (LGR S/N: 10117166, SEN S/N: 10117166, LBL: Température)\",\"Temp., °C (LGR S/N: 10117166, SEN S/N: 10117166, LBL: Température)\",\"Temp., °C (LGR S/N: 10117166, SEN S/N: 10117166, LBL: Température)\",\"Temp., °C (LGR S/N: 10117166, SEN S/N: 10117166, LBL: Température)\",\"Hôte connecté (LGR S/N: 10117166)\",\"Arrêté (LGR S/N: 10117166)\",Fin de fichier (LGR S/N: 10117166) \\n'\n",
    "with open(T_measures, 'w') as file:\n",
    "    for line in all_lines:\n",
    "        file.write(line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "135a7804-b5f5-40aa-8657-4a15774e3666",
    "_uuid": "835cbe0834b935fb0fd40c75b9c39454836f4d5f",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xb0 in position 34: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# read in our data\u001b[39;00m\n\u001b[0;32m      9\u001b[0m capteur_riviere \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../sampling_points/Point034/point034_P_measures.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m capteur_ZH \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../sampling_points/Point034/point034_T_measures.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m etalonage_capteur_riv \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../configuration/pressure_sensors/P508.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# set seed for reproducibility\u001b[39;00m\n",
      "File \u001b[1;32md:\\mines\\maths\\tr\\python\\learn_observe_kkl\\venv\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\mines\\maths\\tr\\python\\learn_observe_kkl\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:586\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    571\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    572\u001b[0m     dialect,\n\u001b[0;32m    573\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    583\u001b[0m )\n\u001b[0;32m    584\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\mines\\maths\\tr\\python\\learn_observe_kkl\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:482\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    479\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    481\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 482\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32md:\\mines\\maths\\tr\\python\\learn_observe_kkl\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:811\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwds:\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 811\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\mines\\maths\\tr\\python\\learn_observe_kkl\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1040\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1037\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown engine: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (valid options are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmapping\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1038\u001b[0m     )\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;66;03m# error: Too many arguments for \"ParserBase\"\u001b[39;00m\n\u001b[1;32m-> 1040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\mines\\maths\\tr\\python\\learn_observe_kkl\\venv\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:69\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     67\u001b[0m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ensure_dtype_objs(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32md:\\mines\\maths\\tr\\python\\learn_observe_kkl\\venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:542\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\mines\\maths\\tr\\python\\learn_observe_kkl\\venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:642\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\mines\\maths\\tr\\python\\learn_observe_kkl\\venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:843\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\mines\\maths\\tr\\python\\learn_observe_kkl\\venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1917\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xb0 in position 34: invalid start byte"
     ]
    }
   ],
   "source": [
    "# modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "# read in our data\n",
    "capteur_riviere = pd.read_csv(\"../sampling_points/Point034/point034_P_measures.csv\")\n",
    "capteur_ZH = pd.read_csv(\"../sampling_points/Point034/point034_T_measures.csv\")\n",
    "etalonage_capteur_riv = pd.read_csv(\"../configuration/pressure_sensors/P508.csv\")\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "604ac3a4-b1d9-4264-b312-4bbeecdeec00",
    "_uuid": "03ce3b4afe87d98f777172c2c7be066a66a0b237"
   },
   "source": [
    "Nous sommes maintenant prêts à examiner quelques dates ! (Si vous le souhaitez, vous pouvez profiter de cette occasion pour jeter un coup d'œil à certaines des données)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imprimez le noms des colonnes\n",
    "renommer les colonnes du fichier capteur_riviere :  \tdates,tension_V,temperature_stream_C et celles du fichier capteur_ZH  :\n",
    "#,dates,temperature_depth_1_C,temperature_depth_2_C,temperature_depth_3_C,temperature_depth_4_C\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformez les données de différence de charge en mètres\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "capteur_riviere.rename(columns = {'Unnamed: 1' : 'dates', 'Unnamed: 2' : 'tension_V', 'Unnamed: 3' : 'temperature_stream_C'}, inplace = True)\n",
    "capteur_ZH.rename(columns = {'Titre de tracé : T520' : '#', 'Date Heure, GMT+01:00': 'dates' , 'Temp., °C (LGR S/N: 10117166, SEN S/N: 10117166, LBL: Température)':'temperature_depth_1_C' ,\n",
    "                             'Temp., °C (LGR S/N: 10117166, SEN S/N: 10117166, LBL: Température).1':'temperature_depth_2_C' , 'Temp., °C (LGR S/N: 10117166, SEN S/N: 10117166, LBL: Température).2':'temperature_depth_3_C' ,\n",
    "                             'Temp., °C (LGR S/N: 10117166, SEN S/N: 10117166, LBL: Température).3':'temperature_depth_4_C'}, inplace = True)\n",
    "print(capteur_riviere.dtypes)\n",
    "# Change types \n",
    "capteur_riviere['tension_V']=pd.to_numeric(capteur_riviere['tension_V'], errors  ='coerce')\n",
    "capteur_riviere['temperature_stream_C']=pd.to_numeric(capteur_riviere['temperature_stream_C'], errors  ='coerce')\n",
    "print(capteur_riviere.dtypes)\n",
    "capteur_riviere.head()\n",
    "capteur_ZH.head()\n",
    "etalonage_capteur_riv.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charge Calculation \n",
    "dUH = pd.to_numeric(etalonage_capteur_riv.loc[3,'P508'])\n",
    "dUT = pd.to_numeric(etalonage_capteur_riv.loc[4,'P508'])\n",
    "intercept = pd.to_numeric(etalonage_capteur_riv.loc[2,'P508'])\n",
    "capteur_riviere['charge_M'] = ((capteur_riviere['tension_V'])-(capteur_riviere['temperature_stream_C'])*dUT-intercept)/(dUH)\n",
    "capteur_riviere['charge_M'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9b87a77d-e5e5-4581-9cd3-0e7339fe1516",
    "_uuid": "742028572a307a42ce40db0102171bc219b05282"
   },
   "source": [
    "# Traitement des dates\n",
    "## Vérifiez le type de données de notre colonne de date\n",
    "___\n",
    "\n",
    "Imprimer les dates des deux fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e6b7eb39-c3e3-40a1-b0a5-91cfcd2d42da",
    "_uuid": "93a08de7a6a621e4b07968c07c1cc612936c6027",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Date impresion\n",
    "print(capteur_riviere['dates'])\n",
    "print(capteur_ZH['dates'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dbdacb7c-10d4-4b0a-8f6b-6d4a940ca446",
    "_uuid": "d88dbc08ab145fd20f86073b027c53f40fd306bc"
   },
   "source": [
    "Remarquez qu'en bas de la sortie de `head()`, vous pouvez voir qu'il est dit que le type de données de cette colonne est \"object\". \n",
    "\n",
    "> Pandas utilise le dtype \"object\" pour stocker différents types de données, mais le plus souvent, lorsque vous voyez une colonne avec le dtype \"object\", elle contient des chaînes de caractères. \n",
    "\n",
    "Si vous consultez la documentation sur le dtype de Pandas [ici] (http://pandas.pydata.org/pandas-docs/stable/basics.html#dtypes), vous remarquerez qu'il existe également un dtype spécifique `datetime64`. Comme le dtype de notre colonne est `object` plutôt que `datetime64`, nous pouvons dire que Python ne sait pas que cette colonne contient des dates.\n",
    "\n",
    "Nous pouvons aussi regarder uniquement le dtype de votre colonne sans imprimer les premières lignes si nous le souhaitons :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "56a047f4-cbf7-4914-951c-a04310ee7432",
    "_uuid": "e2ab2ac80aaac7b165b3af64edb75d29f2612482",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# check the data type of our date column\n",
    "capteur_riviere.dtypes\n",
    "capteur_riviere.dtypes['dates']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "99a207db-3db0-4343-9805-58753f51f6e8",
    "_uuid": "06e6483764014a04e7a1f34525e2f12aee5fdab8"
   },
   "source": [
    "You may have to check the [numpy documentation](https://docs.scipy.org/doc/numpy-1.12.0/reference/generated/numpy.dtype.kind.html#numpy.dtype.kind) to match the letter code to the dtype of the object. \"O\" is the code for \"object\", so we can see that these two methods give us the same information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fb3b552b-411b-4fc0-b1e6-a3a8156fd459",
    "_uuid": "0939ce269aef7001e35cc8f2a5f1eed1f6160940"
   },
   "source": [
    "## Convertir nos colonnes de date en datetime\n",
    "___\n",
    "\n",
    "Maintenant que nous savons que notre colonne de date n'est pas reconnue comme une date, il est temps de la convertir pour qu'elle soit reconnue comme une date. Cette opération est appelée \"analyse syntaxique des dates\" car nous prenons une chaîne de caractères et identifions ses composants. Nous allons utiliser la fonction pd.to_datetime.\n",
    "\n",
    "Nous pouvons indiquer à pandas le format de nos dates à l'aide d'un guide appelé [\"strftime directive\", sur lequel vous trouverez plus d'informations à ce lien] (http://strftime.org/). L'idée de base est que vous devez indiquer quelles parties de la date se trouvent où et quelle ponctuation se trouve entre elles. Il existe [de nombreuses parties possibles d'une date](http://strftime.org/), mais les plus courantes sont `%d` pour le jour, `%m` pour le mois, `%y` pour une année à deux chiffres et `%Y` pour une année à quatre chiffres.\n",
    "\n",
    "Quelques exemples :\n",
    "\n",
    " * Le 17 janvier 2007 a le format \"%m/%d/%y\".\n",
    " * 17-1-2007 a le format \"%d-%m-%Y\".\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f955aa17-ede7-4457-a913-ba1c44f8846d",
    "_uuid": "a471aae50241b245caa0c60fbb19821372682b76",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "capteur_riviere['dates'] = pd.to_datetime(capteur_riviere['dates'] , infer_datetime_format = True, errors = 'coerce')\n",
    "# infer_datetime_format was used because there was diferents formats of datatime\n",
    "capteur_riviere.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7bd8f8b6-8a60-4a12-b94b-4100188845da",
    "_uuid": "fc95b22f0f4d7a6bc0cb1a7cc55abfb204cc81f9"
   },
   "source": [
    "Maintenant que nos dates sont analysées correctement, nous pouvons travailler avec celles-ci.\n",
    "\n",
    "___\n",
    "* Bien que nous spécifions le format de la date ici, il arrive parfois que vous rencontriez une erreur lorsque plusieurs formats de date sont présents dans une même colonne. Dans ce cas, vous pouvez demander à pandas de déduire le bon format de date. Vous pouvez le faire comme suit :\n",
    "\n",
    "`capteur_riviere['date'] = pd.to_datetime(capteur_riviere['date'], infer_datetime_format=True)``\n",
    "\n",
    "**Pourquoi ne pas toujours utiliser `infer_datetime_format = True?`** Il y a deux grandes raisons de ne pas toujours faire deviner à pandas le format de l'heure. La première est que pandas n'est toujours capable de trouver le bon format de date, surtout si quelqu'un a fait preuve de créativité dans la saisie des données. La seconde est que c'est beaucoup plus lent que de spécifier le format exact des dates.\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capteur_ZH['dates'] = pd.to_datetime(capteur_ZH['dates'] , infer_datetime_format = True, errors = 'coerce')\n",
    "capteur_ZH.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fe33df7d-c85d-4b61-b572-5682e6eea81b",
    "_uuid": "a2cec7b480ef13c070d40ca0e0763d2d30a86a9c"
   },
   "source": [
    "## Tracer le jour du mois pour vérifier l'analyse de la date\n",
    "___\n",
    "\n",
    "L'un des plus grands dangers de l'analyse des dates est de mélanger les mois et les jours. La fonction to_datetime() a des messages d'erreur très utiles, mais il n'est pas inutile de vérifier que les jours du mois que nous avons extraits ont un sens. \n",
    "\n",
    "Pour ce faire, nous allons tracer un histogramme des jours du mois. Nous nous attendons à ce qu'il ait des valeurs entre 1 et 31 et, puisqu'il n'y a aucune raison de supposer que les glissements de terrain sont plus fréquents certains jours du mois que d'autres, une distribution relativement égale. (Avec un creux sur 31 car tous les mois n'ont pas 31 jours.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "49feb18f-c077-474e-9353-a24ae850acf6",
    "_uuid": "d3d5a143d3d49e10187e420abfe9cfe18c7bac56",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "capteur_riviere['dates'].apply(lambda x: x.day).hist(bins = 31)\n",
    "# Le trou s'explique par le fait que les données ont éte prises entre le 26 juin et le 12 juillet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "49feb18f-c077-474e-9353-a24ae850acf6",
    "_uuid": "d3d5a143d3d49e10187e420abfe9cfe18c7bac56",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "capteur_ZH['dates'].apply(lambda x: x.day).hist(bins = 31)\n",
    "# idem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valeurs aberrantes\n",
    "\n",
    "L'un des plus grands défis du nettoyage des données est l'identification et le traitement des valeurs aberrantes. En termes simples, les valeurs aberrantes sont des observations qui sont significativement différentes des autres points de données. Même les meilleurs algorithmes d'estimation des paramètres automatique seront moins performants si les observations aberrantes ne sont pas nettoyées des données, car elles impliquent des simulations obsolètes des variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(capteur_riviere.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(capteur_ZH.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identification des valeurs aberrantes\n",
    "\n",
    "La présence de valeurs aberrantes dans les données peut avoir de nombreuses raisons. Parfois, les valeurs aberrantes peuvent être authentiques, alors que dans d'autres cas, elles peuvent être dues à des erreurs de saisie de données. Il est important de comprendre les raisons des valeurs aberrantes avant de les nettoyer.\n",
    "\n",
    "Nous allons commencer le processus de recherche des valeurs aberrantes en exécutant les statistiques sommaires sur les variables. Pour ce faire, nous utilisons la fonction describe() ci-dessous, qui fournit un résumé statistique de toutes les variables quantitatives.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capteur_ZH.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capteur_riviere.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracez les graphiques de distribution pour les différentes variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capteur_riviere.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trouver les valeurs limites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idmax = capteur_riviere['charge_M'].idxmax()\n",
    "idmin = capteur_riviere['charge_M'].idxmin()\n",
    "print('La charge minimale est', capteur_riviere['charge_M'].min(), 'et la charge maximale est', capteur_riviere['charge_M'].max())\n",
    "print('La temperature minimale est', capteur_riviere['temperature_stream_C'].min(), 'et la temperature maximale est', capteur_riviere['temperature_stream_C'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete every value which is greater than a certain threshold\n",
    "capteur_riviere.drop(capteur_riviere.loc[capteur_riviere['charge_M'] > 0.2].index)['charge_M'].hist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capteur_riviere['charge_M'].hist()\n",
    "# Here we see the outliers whit the greatest values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identification des valeurs aberrantes\n",
    "\n",
    "La présence de valeurs aberrantes dans les données peut avoir de nombreuses raisons. Parfois, les valeurs aberrantes peuvent être authentiques, alors que dans d'autres cas, elles peuvent être dues à des erreurs de saisie de données. Il est important de comprendre les raisons des valeurs aberrantes avant de les nettoyer.\n",
    "\n",
    "Nous allons commencer le processus de recherche des valeurs aberrantes en exécutant les statistiques sommaires sur les variables. Pour ce faire, nous utilisons la fonction describe() ci-dessous, qui fournit un résumé statistique de toutes les variables quantitatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracez les graphiques de distribution pour les différentes variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Box plot\n",
    "\n",
    "Le box plot est une manière standardisée d'afficher la distribution des données sur la base du résumé en cinq chiffres (minimum, premier quartile (Q1), médiane, troisième quartile (Q3) et maximum). Elle est souvent utilisée pour identifier la distribution des données et détecter les valeurs aberrantes. La ligne de code ci-dessous trace le box plot de la variable numérique 'Loan_amount'. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capteur_riviere.boxplot(column = ['charge_M'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capteur_riviere.boxplot(column = ['temperature_stream_C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capteur_ZH.boxplot(column = ['temperature_depth_1_C','temperature_depth_2_C','temperature_depth_3_C','temperature_depth_4_C'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Histogramme\n",
    "\n",
    "Un histogramme est utilisé pour visualiser la distribution d'une variable numérique. Une valeur aberrante apparaît en dehors du modèle général de distribution. La ligne de code ci-dessous trace un histogramme de la variable 'Revenu', en utilisant la fonction hist(). A histogram is used to visualize the distribution of a numerical variable. An outlier will appear outside the overall pattern of distribution. The line of code below plots a histogram of the 'Income' variable, using the hist() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capteur_ZH.hist(column = ['temperature_depth_1_C','temperature_depth_2_C','temperature_depth_3_C','temperature_depth_4_C'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scatterplot\n",
    "\n",
    "Un nuage de points permet de visualiser la relation entre deux variables quantitatives. Les données sont affichées sous la forme d'une collection de points, et tout point qui ne correspond pas au regroupement général des deux variables peut indiquer des valeurs aberrantes. \n",
    "\n",
    "génèrez un nuage de points entre les différentes variables de températures. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capteur_ZH.plot.scatter(x = 'temperature_depth_1_C' , y = 'temperature_depth_2_C')\n",
    "capteur_ZH.plot.scatter(x = 'temperature_depth_2_C' , y = 'temperature_depth_3_C')\n",
    "# We can see un obvious outlier in the upper right corner "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identification des valeurs aberrantes avec l'écart interquartile (IQR)\n",
    "\n",
    "L'écart interquartile (IQR) est une mesure de la dispersion statistique et est calculé comme la différence entre les 75e et 25e percentiles. Il est représenté par la formule IQR = Q3 - Q1. Les  lignes de code ci-dessous calculent et impriment l'écart interquartile pour chacune des variables de l'ensemble de données.  La règle générale est que tout ce qui n'est pas dans la plage de (Q1 - 1,5 IQR) et (Q3 + 1,5 IQR) est une valeur aberrante et peut être supprimé.\n",
    "\n",
    "### Fonctions d'assistance\n",
    "\n",
    "Ecrire des fonctions afin d'examiner chaque colonne des deux fichiers de données de terain et de calculer les 1er et 3e quartiles, l'intervalle inter-quartile et le minimum et le maximum. Toute valeur en dehors du minimum et du maximum est une valeur aberrante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_Outliers(data, col, treshold = 1.5):\n",
    "    if type(col) is str :\n",
    "        Q3 = data[col].quantile(0.75)\n",
    "        Q1 = data[col].quantile(0.25)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_range = Q1 - treshold*IQR\n",
    "        upper_range = Q3 + treshold*IQR\n",
    "        outlier_free_list = [x for x in data[col] if ((x > lower_range) & (x < upper_range))]\n",
    "        filtered_data = data.loc[data[col].isin(outlier_free_list)]\n",
    "        return  filtered_data\n",
    "    elif len(col)>1:\n",
    "        filtered_data = remove_Outliers(data, col[0])\n",
    "        for i in col[1:]:\n",
    "            filtered_data = remove_Outliers(filtered_data, i)      \n",
    "        return filtered_data\n",
    " \n",
    "filtered_data = remove_Outliers(capteur_ZH, ['temperature_depth_1_C','temperature_depth_2_C','temperature_depth_3_C','temperature_depth_4_C'])\n",
    "filtered_data.boxplot(column = ['temperature_depth_1_C','temperature_depth_2_C','temperature_depth_3_C','temperature_depth_4_C'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résolution de la distribution asymétrique…\n",
    "\n",
    "A partir de nos données dans un histogramme, nous pouvons obtenir une distribution asymétrique des données. Une seule grande barre avec de minuscules barres à gauche ou à droite (ou les deux) est un signe révélateur que des valeurs aberrantes peuvent être présentes dans les données et cela signifie que notre bel histogramme, bien rangé et normalement distribué, est complètement caché et obscurci par le seul grand bar.\n",
    "\n",
    "\n",
    "Lorsque nous observons ce modèle, nous devons supprimer les valeurs aberrantes, puis voir à quoi ressemble la nouvelle distribution. Si nous voulons vérifier la présence de valeurs aberrantes, une boîte à moustaches rapide confirmera ou refusera…\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est ce que l'on a déjà fait dans la partie \"Trouver la valeur limite\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Suppression des valeurs aberrantes et visualisation du résultat\n",
    "\n",
    "Après avoir fait tout le gros du travail dans les fonctions d'assistance, nous pouvons maintenant supprimer les lignes des données qui contiennent des valeurs aberrantes en dehors de | - et - | moustaches…\n",
    "\n",
    "\n",
    "\n",
    "## Expliquez le résultat\n",
    "est ce que cette méthodologie vous semble suffisante? Tentez de trouver les limites de cette méthodologie?\n",
    "Que pouvez vous apportez aux fonctions afin de les améliorer? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une limite de cette méthodologie est qu'elle est arbitraire et pas adaptable : tous les points très loin de la mediane ne sont pas forcement aberrants. C'est pourquoi le paramètre 1.5 est devenue une variable de la function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valeurs abérantes avec la méthode score Z \n",
    "Il s'agit d'une unité mesurée en écart-type. Fondamentalement, il s'agit d'une mesure de la distance entre le score brut et la moyenne. Il est compris entre -3 et +3, où 0 = moyenne et 1 = écart-type, c'est-à-dire une distribution normale.\n",
    "\n",
    "Cette méthode suggère qu'en tout point de données il existe une relation entre l'écart-type et la moyenne. Le score Z permet de trouver la distribution des données où la moyenne est égale à 0 et l'écart-type à 1, c'est-à-dire une distribution normale. Vous devez vous demander en quoi cela peut nous aider à identifier les valeurs aberrantes ? En calculant le score Z, nous redimensionnons et centrons les données et recherchons les points de données qui sont trop éloignés de zéro. Ces points de données qui sont trop éloignés de zéro seront traités comme des valeurs aberrantes. Dans la plupart des cas, un seuil de 3 ou -3 est utilisé, c'est-à-dire que si la valeur du score Z est supérieure ou inférieure à 3 ou -3 respectivement, ce point de données sera identifié comme aberrant.\n",
    "\n",
    "Formule pour le score Z = (Observation - Moyenne)/Écart-type\n",
    "\n",
    "z = (X - μ) / σ\n",
    "\n",
    "Ecrire des functions afin de pouvoir appliquer cette méthodologie aux données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_Z_outliers(data, col, treshold = 3):\n",
    "    if type(col) is str :\n",
    "        mean = data[col].mean()\n",
    "        std = data[col].std()\n",
    "        outlier_free_list = [x for x in data[col] if (np.abs(x-mean)/std <treshold)]\n",
    "        filtered_data = data.loc[data[col].isin(outlier_free_list)]\n",
    "        return  filtered_data\n",
    "    else:\n",
    "        filtered_data = remove_Z_outliers(data, col[0])\n",
    "        for i in col[1:]:\n",
    "            filtered_data = remove_Z_outliers(filtered_data, i)       \n",
    "        return filtered_data\n",
    "\n",
    "res = remove_Z_outliers(capteur_ZH, ['temperature_depth_1_C','temperature_depth_1_C'])\n",
    "res.boxplot( 'temperature_depth_1_C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capteur_ZH.boxplot( 'temperature_depth_1_C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.plot.scatter(x = 'temperature_depth_1_C' , y = 'temperature_depth_2_C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparer les deux méthodologies, donner leurs avantages et leurs limitations\n",
    "Vous pouvez proposer des alternatives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut faire le même commentaire pour la méthodologie Z-score. Elle est arbitraire et pas adaptable : tous les points très loin de la mediane ne sont pas forcement aberrants. C'est pourquoi le paramètre 3 est devenue une variable de la function. \n",
    "Le Z-score a tout de même comme avantage de préserver 99% des valeures d'une gaussiene. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Proposer une fonction permettant d'aller des données brutes jusqu'au premier traitement des données.\n",
    "\n",
    "N'oubliez pas de permettre à l'utilisateur de pouvoir prendre des décisions et de l'aider à prendre ces décisions\n",
    "Pensez à indiquer à l'utilisateur si le jeu de données est utilisable, quels sont les capteurs défaillant, quel semble être l'orientation des échanges nappe-rivière, les données de différence de charge semblent t'elles données les mêmes informations que les données du profils de température.\n",
    "\n",
    "Votre function doit prendre en entrée, les trois fichiers et proposer au fur et à mesure de son dérouler des propositions à l'utilisateur.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def upper_cut(data, col, treshold = 0.2):\n",
    "    if type(col) is str :\n",
    "        res = data.drop(data.loc[data[col] > treshold].index)\n",
    "        return res\n",
    "    elif len(col)>1:\n",
    "        filtered_data = upper_cut(data, col[0], treshold)\n",
    "        for i in col[1:]:\n",
    "            filtered_data = upper_cut(filtered_data, i, treshold)    \n",
    "        return filtered_data\n",
    "\n",
    "def under_cut(data, col, treshold = 0.2):\n",
    "    if type(col) is str :\n",
    "        res = data.drop(data.loc[data[col] < treshold].index)\n",
    "        return res\n",
    "    elif len(col)>1:\n",
    "        filtered_data = upper_cut(data, col[0], treshold)\n",
    "        for i in col[1:]:\n",
    "            filtered_data = upper_cut(filtered_data, i, treshold)    \n",
    "        return filtered_data\n",
    "    \n",
    "def processing(T_measures = \"../sampling_points/Point034/point034_T_measures.csv\",\n",
    "               P_measures =\"../sampling_points/Point034/point034_P_measures.csv\",\n",
    "               config = \"../configuration/pressure_sensors/P508.csv\"):\n",
    "    with open(T_measures, 'r') as file:\n",
    "        all_lines = file.readlines()\n",
    "    file.close()\n",
    "    all_lines[0] = '#,\"Date Heure, GMT+01:00\",\"Temp., °C (LGR S/N: 10117166, SEN S/N: 10117166, LBL: Température)\",\"Temp., °C (LGR S/N: 10117166, SEN S/N: 10117166, LBL: Température)\",\"Temp., °C (LGR S/N: 10117166, SEN S/N: 10117166, LBL: Température)\",\"Temp., °C (LGR S/N: 10117166, SEN S/N: 10117166, LBL: Température)\",\"Hôte connecté (LGR S/N: 10117166)\",\"Arrêté (LGR S/N: 10117166)\",Fin de fichier (LGR S/N: 10117166)'  \n",
    "    with open(T_measures, 'w') as file:\n",
    "        for line in all_lines:\n",
    "            file.write(line)\n",
    "    capteur_riviere = pd.read_csv(P_measures)\n",
    "    capteur_ZH = pd.read_csv(T_measures)\n",
    "    etalonage_capteur_riv = pd.read_csv(config)\n",
    "    np.random.seed(0)\n",
    "    capteur_ZH.head()\n",
    "    capteur_riviere.rename(columns = {'Unnamed: 1' : 'dates', 'Unnamed: 2' : 'tension_V', 'Unnamed: 3' : 'temperature_stream_C'}, inplace = True)\n",
    "    capteur_ZH.rename(columns = {'Titre de tracé : T520' : '#', 'Date Heure, GMT+01:00': 'dates' , 'Temp., °C (LGR S/N: 10117166, SEN S/N: 10117166, LBL: Température)':'temperature_depth_1_C' , 'Temp., °C (LGR S/N: 10117166, SEN S/N: 10117166, LBL: Température).1':'temperature_depth_2_C' , 'Temp., °C (LGR S/N: 10117166, SEN S/N: 10117166, LBL: Température).2':'temperature_depth_3_C' , 'Temp., °C (LGR S/N: 10117166, SEN S/N: 10117166, LBL: Température).3':'temperature_depth_4_C'}, inplace = True)\n",
    "    print(capteur_riviere.dtypes)\n",
    "    capteur_riviere['tension_V']=pd.to_numeric(capteur_riviere['tension_V'], errors  ='coerce')\n",
    "    capteur_riviere['temperature_stream_C']=pd.to_numeric(capteur_riviere['temperature_stream_C'], errors  ='coerce')\n",
    "    print(capteur_riviere.dtypes)\n",
    "  \n",
    "    etalonage_capteur_riv.dtypes\n",
    "    dUH = pd.to_numeric(etalonage_capteur_riv.loc[3,'P508'])\n",
    "    dUT = pd.to_numeric(etalonage_capteur_riv.loc[4,'P508'])\n",
    "    intercept = pd.to_numeric(etalonage_capteur_riv.loc[2,'P508'])\n",
    "    capteur_riviere['charge_M'] = ((capteur_riviere['tension_V'])-(capteur_riviere['temperature_stream_C'])*dUT-intercept)/(dUH)\n",
    "    capteur_riviere['dates'] = pd.to_datetime(capteur_riviere['dates'] , infer_datetime_format = True, errors = 'coerce')\n",
    "    capteur_ZH['dates'] = pd.to_datetime(capteur_ZH['dates'] , infer_datetime_format = True, errors = 'coerce')\n",
    "    print(capteur_riviere.info())\n",
    "    print(capteur_ZH.info())\n",
    "    capteur_ZH.describe()\n",
    "    capteur_riviere.describe()\n",
    "    capteur_riviere['temperature_stream_C'].hist()\n",
    "    print('La charge minimale est', capteur_riviere['charge_M'].min(), 'et la charge maximale est', capteur_riviere['charge_M'].max())\n",
    "    print('La temperature minimale est', capteur_riviere['temperature_stream_C'].min(), 'et la temperature maximale est', capteur_riviere['temperature_stream_C'].max())\n",
    "    print(\"Raw data\")\n",
    "    capteur_riviere['charge_M'].hist()\n",
    "    capteur_riviere.boxplot(column = ['charge_M'])\n",
    "    capteur_riviere.boxplot(column = ['temperature_stream_C'])\n",
    "    #capteur_ZH.boxplot(column = ['temperature_depth_1_C','temperature_depth_2_C','temperature_depth_3_C','temperature_depth_4_C'])\n",
    "    capteur_ZH.hist(column = ['temperature_depth_1_C','temperature_depth_2_C','temperature_depth_3_C','temperature_depth_4_C'])\n",
    "    capteur_ZH.plot.scatter(x = 'temperature_depth_1_C' , y = 'temperature_depth_2_C')\n",
    "    capteur_ZH.plot.scatter(x = 'temperature_depth_2_C' , y = 'temperature_depth_3_C')\n",
    "    def id(data, col, treshold):\n",
    "        return data\n",
    "    mode = input(\"Remove temp outlier method (None, up_cut, un_cut, Z_score, IQ) :\" )\n",
    "    methods = {'up_cut' : upper_cut, 'un_cut' : under_cut, 'Z_score' : remove_Z_outliers, 'IQ' : remove_Outliers, 'None' : id}\n",
    "    treshold = input(\"Threshold (press enter for default):\")\n",
    "    if treshold == '':\n",
    "        filtered_temp = methods[mode](capteur_ZH, col =  ['temperature_depth_1_C','temperature_depth_2_C','temperature_depth_3_C','temperature_depth_4_C'])\n",
    "    else:\n",
    "        treshold = int(treshold)\n",
    "        filtered_temp = methods[mode](capteur_ZH, col =  ['temperature_depth_1_C','temperature_depth_2_C','temperature_depth_3_C','temperature_depth_4_C'], treshold = treshold)\n",
    "    \n",
    "    #filtered_temp.boxplot(column = ['temperature_depth_1_C','temperature_depth_2_C','temperature_depth_3_C','temperature_depth_4_C'])\n",
    "    filtered_temp.hist(column = ['temperature_depth_1_C','temperature_depth_2_C','temperature_depth_3_C','temperature_depth_4_C'])\n",
    "    filtered_temp.plot.scatter(x = 'temperature_depth_1_C' , y = 'temperature_depth_2_C')\n",
    "    filtered_temp.plot.scatter(x = 'temperature_depth_2_C' , y = 'temperature_depth_3_C')\n",
    "    \n",
    "    mode = input(\"Remove Pressure outlier method (None, up_cut, un_cut, Z_score, IQ) :\" )\n",
    "\n",
    "    treshold = input(\"Threshold (press enter for default):\")\n",
    "    if treshold == '':\n",
    "        filtered_p = methods[mode](capteur_riviere, col =  ['temperature_stream_C','charge_M'])\n",
    "    else:\n",
    "        treshold = int(treshold)\n",
    "        filtered_p = methods[mode](capteur_riviere, col =  ['temperature_stream_C','charge_M'], treshold = treshold)\n",
    "    filtered_p['charge_M'].hist()\n",
    "    filtered_p.boxplot(column = ['charge_M'])\n",
    "    filtered_p.boxplot(column = ['temperature_stream_C'])\n",
    "    return filtered_p,filtered_temp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Deux autres jeux de données sont disponibles sur le répertoire github.\n",
    "Votre rôle est de traiter et d'analyser ces données avec la méthodologie que vous avez développé."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
